{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "from inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "import matplotlib.image as mpimg\n",
    "from dlproject import get_label_mapping, get_files\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_file = './inception_resnet_v2_2016_08_30.ckpt'\n",
    "label_file = './labels.txt'\n",
    "train_file = 'Data/'\n",
    "validation_file = 'validation/'\n",
    "bottle_neck_file = 'bottle_neck/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=mpimg.imread(train_file + '/a/0.jpg')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.placeholder(tf.float32, shape=(None, 480, 720, 3))\n",
    "labels = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "# restore from the inception_resnet model\n",
    "with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "        _, end_points = inception_resnet_v2(images, num_classes = 1001, is_training = False) \n",
    "\n",
    "# extract the before_logit tensor from the extracted model\n",
    "before_logit = end_points['PreLogitsFlatten']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a placeholder for training from the bottle, not from the beginning.\n",
    "bottle_neck = tf.placeholder(tf.float32, shape=(None,1536))\n",
    "\n",
    "# now define the fine tune part of the graph\n",
    "with tf.name_scope('my_fine_tune') as scope:\n",
    "    batch_norm1 = tf.layers.batch_normalization(bottle_neck, name='batch_norm1')\n",
    "    dropout1 = tf.layers.dropout(batch_norm1, rate=0.3, name='dropout1')\n",
    "    dense1 = tf.layers.dense(dropout1, 128, activation=tf.nn.relu, name='dense1')\n",
    "    batch_norm2 = tf.layers.batch_normalization(dense1, name='batch_norm2')\n",
    "    dropout2 = tf.layers.dropout(batch_norm2, rate = 0.3, name='dropout2')\n",
    "    logits = tf.layers.dense(dropout2, 3, name='logits')\n",
    "    \n",
    "    x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(x_entropy, name='loss')\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    # extract variables that need to be trained\n",
    "    weight1, bias1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'dense1')\n",
    "    weight2, bias2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'logits')\n",
    "    variables_to_train = [weight1, weight2, bias1, bias2]\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = slim.learning.create_train_op(loss, optimizer, variables_to_train=variables_to_train)\n",
    "\n",
    "# defiine the name scope we don't need to restore from inception_resnet\n",
    "exclude = ['InceptionResnetV2/AuxLogits', 'InceptionResnetV2/Logits',\n",
    "           'my_fine_tune/','dense1','logits','batch_norm']\n",
    "\n",
    "variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n",
    "saver = tf.train.Saver(variables_to_restore)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the data before the fine tune layer\n",
    "def get_bottle_neck_data(sess, train_file, bottle_neck_file, label_file):\n",
    "    id2label, label2id = get_label_mapping(label_file)\n",
    "    for i in id2label:\n",
    "        read_dir = train_file + i\n",
    "        des_dir = bottle_neck_file + i + '/'\n",
    "        os.mkdir(des_dir)\n",
    "        \n",
    "        files = get_files(read_dir)\n",
    "        for f in files:\n",
    "            img=mpimg.imread(f).reshape(1,480,720,3)\n",
    "            bottle = sess.run(before_logit, feed_dict={images:img})\n",
    "            np.save(des_dir + f[7:],  bottle)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement = True\n",
    "\n",
    "def train_graph():\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "        \n",
    "        # if there is no dir for the bottle data, then get the data first.\n",
    "        if(os.path.isdir(bottle_neck_file) is not True):\n",
    "            os.mkdir(bottle_neck_file)\n",
    "            get_bottle_neck_data(sess, train_file, bottle_neck_file, label_file)\n",
    "        \n",
    "#         for i in range(10000):\n",
    "#             inputs, groud_truth = *****.next_batch()\n",
    "#             the_loss = sess.run([train_op], feed_dict={bottle_neck: inputs, labels: groud_truth})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
