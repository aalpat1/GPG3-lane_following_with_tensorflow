{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "from inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "import matplotlib.image as mpimg\n",
    "from dlproject import get_label_mapping, get_files, bottle_neck_process\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_file = './inception_resnet_v2_2016_08_30.ckpt'\n",
    "label_file = './labels.txt'\n",
    "train_file = 'Data/'\n",
    "bottle_neck_file = './bottle_neck/'\n",
    "save_model_file = './model_file/inception_resnet_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 720, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=mpimg.imread(train_file + '/a/0.jpg')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = tf.placeholder(tf.float32, shape=(None, 480, 720, 3))\n",
    "labels = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "# restore from the inception_resnet model\n",
    "with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "        _, end_points = inception_resnet_v2(images, num_classes = 1001, is_training = False) \n",
    "\n",
    "# extract the before_logit tensor from the extracted model\n",
    "before_logit = end_points['PreLogitsFlatten']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a placeholder for training from the bottle, not from the beginning.\n",
    "bottle_neck = tf.placeholder(tf.float32, shape=(None,1536))\n",
    "\n",
    "# now define the fine tune part of the graph\n",
    "with tf.name_scope('my_fine_tune') as scope:\n",
    "    batch_norm1 = tf.layers.batch_normalization(bottle_neck, name='batch_norm1')\n",
    "    dropout1 = tf.layers.dropout(batch_norm1, rate=0.3, name='dropout1')\n",
    "    dense1 = tf.layers.dense(dropout1, 128, activation=tf.nn.relu, name='dense1')\n",
    "    batch_norm2 = tf.layers.batch_normalization(dense1, name='batch_norm2')\n",
    "    dropout2 = tf.layers.dropout(batch_norm2, rate = 0.3, name='dropout2')\n",
    "    logits = tf.layers.dense(dropout2, 4, name='logits')\n",
    "    \n",
    "    x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(x_entropy, name='loss')\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    # extract variables that need to be trained\n",
    "    weight1, bias1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'dense1')\n",
    "    weight2, bias2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'logits')\n",
    "    variables_to_train = [weight1, weight2, bias1, bias2]\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = slim.learning.create_train_op(loss, optimizer, variables_to_train=variables_to_train)\n",
    "\n",
    "# defiine the name scope we don't need to restore from inception_resnet\n",
    "exclude = ['InceptionResnetV2/AuxLogits', 'InceptionResnetV2/Logits',\n",
    "           'my_fine_tune/','dense1','logits','batch_norm']\n",
    "\n",
    "variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n",
    "saver = tf.train.Saver(variables_to_restore)\n",
    "init = tf.global_variables_initializer()\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(save_model_file)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the data before the fine tune layer\n",
    "def get_bottle_neck_data(sess, train_file, bottle_neck_file, label_file):\n",
    "    id2label, label2id = get_label_mapping(label_file)\n",
    "    for i in id2label:\n",
    "        read_dir = train_file + i\n",
    "        des_dir = bottle_neck_file + i + '/'\n",
    "        os.mkdir(des_dir)\n",
    "        \n",
    "        files = get_files(read_dir)\n",
    "        for f in files:\n",
    "            img=mpimg.imread(f).reshape(1,480,720,3) / 255\n",
    "            bottle = sess.run(before_logit, feed_dict={images:img})\n",
    "            np.save(des_dir + f[7:],  bottle)\n",
    "            print('save the bottle neck file ' + f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_graph(sess):\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, checkpoint_file)\n",
    "\n",
    "    # if there is no dir for the bottle data, then get the data first.\n",
    "    if(os.path.isdir(bottle_neck_file) is not True):\n",
    "        os.mkdir(bottle_neck_file)\n",
    "        get_bottle_neck_data(sess, train_file, bottle_neck_file, label_file)\n",
    "\n",
    "    # get training data from bottle_neck file, and train.\n",
    "    bottle_neck_processor = bottle_neck_process(bottle_neck_file, label_file) \n",
    "    for i in range(1500):\n",
    "        inputs, groud_truth = bottle_neck_processor.next_batch(128)\n",
    "        train_loss, train_accuracy = sess.run([train_op, accuracy], feed_dict={bottle_neck: inputs, \n",
    "                                                                             labels: groud_truth})\n",
    "\n",
    "        if(i % 50 == 0):\n",
    "            val_loss, val_accuracy = sess.run([loss, accuracy],\n",
    "                                              feed_dict={bottle_neck: bottle_neck_processor.val_inputs,\n",
    "                                                         labels: bottle_neck_processor.val_labels})\n",
    "        \n",
    "            print('####################################')\n",
    "            print('the training loss after ' + str(i) + ' iterations is: ' + str(train_loss))\n",
    "            print('the training accuracy after ' + str(i) + ' iterations is: ' + str(train_accuracy))\n",
    "            print('the val loss after ' + str(i) + ' iterations is: ' + str(val_loss))\n",
    "            print('the val accuracy after ' + str(i) + ' iterations is: ' + str(val_accuracy))\n",
    "        \n",
    "#         after the training process finished, store the model to a '.pb' version file.\n",
    "    builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.TRAINING],\n",
    "                                         signature_def_map=None, assets_collection=None)\n",
    "    \n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./inception_resnet_v2_2016_08_30.ckpt\n",
      "####################################\n",
      "the training loss after 0 iterations is: 1.3148\n",
      "the training accuracy after 0 iterations is: 0.547619\n",
      "the val loss after 0 iterations is: 0.981747\n",
      "the val accuracy after 0 iterations is: 0.631336\n",
      "####################################\n",
      "the training loss after 50 iterations is: 0.655112\n",
      "the training accuracy after 50 iterations is: 0.722222\n",
      "the val loss after 50 iterations is: 0.521982\n",
      "the val accuracy after 50 iterations is: 0.769585\n",
      "####################################\n",
      "the training loss after 100 iterations is: 0.506532\n",
      "the training accuracy after 100 iterations is: 0.777778\n",
      "the val loss after 100 iterations is: 0.464972\n",
      "the val accuracy after 100 iterations is: 0.788018\n",
      "####################################\n",
      "the training loss after 150 iterations is: 0.412533\n",
      "the training accuracy after 150 iterations is: 0.84127\n",
      "the val loss after 150 iterations is: 0.450595\n",
      "the val accuracy after 150 iterations is: 0.801843\n",
      "####################################\n",
      "the training loss after 200 iterations is: 0.39401\n",
      "the training accuracy after 200 iterations is: 0.833333\n",
      "the val loss after 200 iterations is: 0.447118\n",
      "the val accuracy after 200 iterations is: 0.792627\n",
      "####################################\n",
      "the training loss after 250 iterations is: 0.29522\n",
      "the training accuracy after 250 iterations is: 0.888889\n",
      "the val loss after 250 iterations is: 0.430183\n",
      "the val accuracy after 250 iterations is: 0.824885\n",
      "####################################\n",
      "the training loss after 300 iterations is: 0.276192\n",
      "the training accuracy after 300 iterations is: 0.936508\n",
      "the val loss after 300 iterations is: 0.436933\n",
      "the val accuracy after 300 iterations is: 0.801843\n",
      "####################################\n",
      "the training loss after 350 iterations is: 0.32002\n",
      "the training accuracy after 350 iterations is: 0.888889\n",
      "the val loss after 350 iterations is: 0.412921\n",
      "the val accuracy after 350 iterations is: 0.820276\n",
      "####################################\n",
      "the training loss after 400 iterations is: 0.279235\n",
      "the training accuracy after 400 iterations is: 0.880952\n",
      "the val loss after 400 iterations is: 0.434723\n",
      "the val accuracy after 400 iterations is: 0.824885\n",
      "####################################\n",
      "the training loss after 450 iterations is: 0.23338\n",
      "the training accuracy after 450 iterations is: 0.912698\n",
      "the val loss after 450 iterations is: 0.429882\n",
      "the val accuracy after 450 iterations is: 0.81106\n",
      "####################################\n",
      "the training loss after 500 iterations is: 0.201397\n",
      "the training accuracy after 500 iterations is: 0.952381\n",
      "the val loss after 500 iterations is: 0.444362\n",
      "the val accuracy after 500 iterations is: 0.820276\n",
      "####################################\n",
      "the training loss after 550 iterations is: 0.198656\n",
      "the training accuracy after 550 iterations is: 0.960317\n",
      "the val loss after 550 iterations is: 0.435582\n",
      "the val accuracy after 550 iterations is: 0.815668\n",
      "####################################\n",
      "the training loss after 600 iterations is: 0.174325\n",
      "the training accuracy after 600 iterations is: 0.944444\n",
      "the val loss after 600 iterations is: 0.412311\n",
      "the val accuracy after 600 iterations is: 0.824885\n",
      "####################################\n",
      "the training loss after 650 iterations is: 0.275045\n",
      "the training accuracy after 650 iterations is: 0.880952\n",
      "the val loss after 650 iterations is: 0.420787\n",
      "the val accuracy after 650 iterations is: 0.834101\n",
      "####################################\n",
      "the training loss after 700 iterations is: 0.150747\n",
      "the training accuracy after 700 iterations is: 0.944444\n",
      "the val loss after 700 iterations is: 0.478399\n",
      "the val accuracy after 700 iterations is: 0.820276\n",
      "####################################\n",
      "the training loss after 750 iterations is: 0.219094\n",
      "the training accuracy after 750 iterations is: 0.920635\n",
      "the val loss after 750 iterations is: 0.454416\n",
      "the val accuracy after 750 iterations is: 0.81106\n",
      "####################################\n",
      "the training loss after 800 iterations is: 0.140433\n",
      "the training accuracy after 800 iterations is: 0.968254\n",
      "the val loss after 800 iterations is: 0.427129\n",
      "the val accuracy after 800 iterations is: 0.83871\n",
      "####################################\n",
      "the training loss after 850 iterations is: 0.16355\n",
      "the training accuracy after 850 iterations is: 0.928571\n",
      "the val loss after 850 iterations is: 0.440312\n",
      "the val accuracy after 850 iterations is: 0.83871\n",
      "####################################\n",
      "the training loss after 900 iterations is: 0.131985\n",
      "the training accuracy after 900 iterations is: 0.960317\n",
      "the val loss after 900 iterations is: 0.482038\n",
      "the val accuracy after 900 iterations is: 0.834101\n",
      "####################################\n",
      "the training loss after 950 iterations is: 0.138467\n",
      "the training accuracy after 950 iterations is: 0.968254\n",
      "the val loss after 950 iterations is: 0.436182\n",
      "the val accuracy after 950 iterations is: 0.829493\n",
      "####################################\n",
      "the training loss after 1000 iterations is: 0.129402\n",
      "the training accuracy after 1000 iterations is: 0.952381\n",
      "the val loss after 1000 iterations is: 0.489704\n",
      "the val accuracy after 1000 iterations is: 0.820276\n",
      "####################################\n",
      "the training loss after 1050 iterations is: 0.144414\n",
      "the training accuracy after 1050 iterations is: 0.960317\n",
      "the val loss after 1050 iterations is: 0.453347\n",
      "the val accuracy after 1050 iterations is: 0.820276\n",
      "####################################\n",
      "the training loss after 1100 iterations is: 0.0982733\n",
      "the training accuracy after 1100 iterations is: 0.984127\n",
      "the val loss after 1100 iterations is: 0.475428\n",
      "the val accuracy after 1100 iterations is: 0.829493\n",
      "####################################\n",
      "the training loss after 1150 iterations is: 0.133269\n",
      "the training accuracy after 1150 iterations is: 0.968254\n",
      "the val loss after 1150 iterations is: 0.482747\n",
      "the val accuracy after 1150 iterations is: 0.83871\n",
      "####################################\n",
      "the training loss after 1200 iterations is: 0.108926\n",
      "the training accuracy after 1200 iterations is: 0.968254\n",
      "the val loss after 1200 iterations is: 0.492141\n",
      "the val accuracy after 1200 iterations is: 0.824885\n",
      "####################################\n",
      "the training loss after 1250 iterations is: 0.0908244\n",
      "the training accuracy after 1250 iterations is: 0.984127\n",
      "the val loss after 1250 iterations is: 0.511202\n",
      "the val accuracy after 1250 iterations is: 0.806452\n",
      "####################################\n",
      "the training loss after 1300 iterations is: 0.100888\n",
      "the training accuracy after 1300 iterations is: 0.984127\n",
      "the val loss after 1300 iterations is: 0.565995\n",
      "the val accuracy after 1300 iterations is: 0.815668\n",
      "####################################\n",
      "the training loss after 1350 iterations is: 0.0805954\n",
      "the training accuracy after 1350 iterations is: 0.97619\n",
      "the val loss after 1350 iterations is: 0.526099\n",
      "the val accuracy after 1350 iterations is: 0.829493\n",
      "####################################\n",
      "the training loss after 1400 iterations is: 0.0643836\n",
      "the training accuracy after 1400 iterations is: 0.984127\n",
      "the val loss after 1400 iterations is: 0.52684\n",
      "the val accuracy after 1400 iterations is: 0.820276\n",
      "####################################\n",
      "the training loss after 1450 iterations is: 0.0732695\n",
      "the training accuracy after 1450 iterations is: 0.984127\n",
      "the val loss after 1450 iterations is: 0.517863\n",
      "the val accuracy after 1450 iterations is: 0.81106\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'./model_file/inception_resnet_v2/saved_model.pb'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    train_graph(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottle_neck_processor = bottle_neck_process(bottle_neck_file, label_file) \n",
    "inputs, groud_truth = bottle_neck_processor.next_batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 1536)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groud_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
